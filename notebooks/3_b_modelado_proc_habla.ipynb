{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTpu7pHup5pq"
      },
      "source": [
        "## **Estructura de sub-etapas del procesamiento del habla:**\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "├── Input Layer\n",
        "│   ├── Audio Capture\n",
        "│   └── ASR (Speech-to-Text)\n",
        "├── NLP Layer\n",
        "│   ├── Text Preprocessing\n",
        "│   ├── Intent Detection\n",
        "│   ├── Entity Recognition\n",
        "│   └── Sentiment Analysis\n",
        "├── Dialogue Management\n",
        "│   ├── State Management\n",
        "│   ├── Response Generation\n",
        "│   ├── Response Selection\n",
        "│   └── Context Handling\n",
        "├── Output Layer\n",
        "│   ├── TTS (Text-to-Speech)\n",
        "│   └── Audio Playback\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba_DUci1CkX6"
      },
      "source": [
        "### **Estructura operativa para cada sub-etapa:**\n",
        "\n",
        "**Selección de técnicas de modelado:** En este caso se utilizara [] para la etapa de reconocimiento de voz, [] para el procesamiento y analisis de texto y [] para la conversion de texto a voz.\n",
        "\n",
        "**Generación de un diseño de comprobación:** Para elegir el modelo correcto, este devera ser el que pondere mas alto en un promedio de las diferentes metricas de evaluacion del modelo.\n",
        "\n",
        "**Generación de los modelos:** Se definiran y configuraran los parametros del modelo para pasar a la etapa de ejecucion y descripcion.\n",
        "\n",
        "**Evaluación del modelo:** Se evaluaran los diferentes modelos de manera individual en busqueda de optimizar sus parametros y escoger la combinacion mas optima entre modelos / parametros individuales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM_Jd-w3lOWe"
      },
      "outputs": [],
      "source": [
        "# Execute if use colab and you need import files\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute if use colab and you need import files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTkfaLx7boQi"
      },
      "outputs": [],
      "source": [
        "# Execute if use colab\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install python3\n",
        "apt-get install -venv\n",
        "python3 -m venv proc_habla\n",
        "source proc_habla/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and activate virtual env for speech processing stage\n",
        "!python3 -m venv proc_habla\n",
        "!source proc_habla/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MT6XY2qoIA"
      },
      "source": [
        "## **1. Entrada de Usuario (Reconocimiento del Habla)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "yIsbz7_HkZZJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.10.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Users/cristianariel/Library/Python/3.11/lib/python/site-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from SpeechRecognition) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cristianariel/Library/Python/3.11/lib/python/site-packages (from requests>=2.26.0->SpeechRecognition) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/cristianariel/Library/Python/3.11/lib/python/site-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cristianariel/Library/Python/3.11/lib/python/site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/cristianariel/Library/Python/3.11/lib/python/site-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
            "Requirement already satisfied: sounddevice in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.4.7)\n",
            "Requirement already satisfied: CFFI>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sounddevice) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.11.1)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scipy) (1.25.2)\n",
            "Requirement already satisfied: pydub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.25.1)\n",
            "Requirement already satisfied: pocketsphinx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (5.0.3)\n",
            "Requirement already satisfied: sounddevice in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pocketsphinx) (0.4.7)\n",
            "Requirement already satisfied: CFFI>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sounddevice->pocketsphinx) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install SpeechRecognition\n",
        "!pip3 install sounddevice\n",
        "!pip3 install scipy\n",
        "!pip3 install pydub\n",
        "!pip install pocketsphinx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "eQgslv4y1nrF"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "import numpy as np\n",
        "import wave\n",
        "import pandas as pd\n",
        "import sounddevice as sd\n",
        "from scipy.io.wavfile import write\n",
        "import queue\n",
        "import threading\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ruta del archivo zip descargado\n",
        "ruta_modelo_comprimido_pocketsphinx_esp = \"../datos/brutos/modelos_proc_habla/modelo_esp.zip\"\n",
        "\n",
        "# Directorio donde se colocarán los archivos descomprimidos\n",
        "ruta_modelo_descomprimido_pocketsphinx_esp = \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/pocketsphinx-data/es-ES/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos descomprimidos en: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/pocketsphinx-data/es-ES/\n"
          ]
        }
      ],
      "source": [
        "# Descomprimir el archivo zip\n",
        "with zipfile.ZipFile(ruta_modelo_comprimido_pocketsphinx_esp, 'r') as zip_ref:\n",
        "    zip_ref.extractall(ruta_modelo_descomprimido_pocketsphinx_esp)\n",
        "\n",
        "print(\"Archivos descomprimidos en:\", ruta_modelo_descomprimido_pocketsphinx_esp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
            "sudo: a password is required\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFmpeg instalado exitosamente en sistemas basados en Debian.\n",
            "FFmpeg instalado exitosamente en macOS utilizando Homebrew.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: ffmpeg 7.0.1 is already installed and up-to-date.\n",
            "To reinstall 7.0.1, run:\n",
            "  brew reinstall ffmpeg\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    subprocess.run([\"sudo\", \"apt-get\", \"install\", \"ffmpeg\"])\n",
        "    print(\"FFmpeg instalado exitosamente en sistemas basados en Debian.\")\n",
        "except Exception as e:\n",
        "    print(\"Error al instalar FFmpeg:\", e)\n",
        "try:\n",
        "    subprocess.run([\"brew\", \"install\", \"ffmpeg\"])\n",
        "    print(\"FFmpeg instalado exitosamente en macOS utilizando Homebrew.\")\n",
        "except Exception as e:\n",
        "    print(\"Error al instalar FFmpeg:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxPsnrV6riGR"
      },
      "source": [
        "## **Módulo de Captura de Audio:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Bugs*\n",
        "\n",
        "- Recibo algun id de consulta, cliente? (lo necesito para gestionar la consulta de manera eficiente)\n",
        "- Definir como se llamara a la funcion chatbot\n",
        "    - Definir el anidamiento de funciones, para que con solo llamar a chatbot, me entregue el resultado (lista de productos limpia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "8bLph4ehd_0_"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 44100  # Tasa de muestreo\n",
        "DURATION = 10  # Duración de la grabación en segundos\n",
        "AUDIO_FILES_PATH = \"../datos/brutos/audios_proc_habla/\"\n",
        "FILENAME = \"output\"  # Nombre del archivo de salida\n",
        "EXTENCION_ENTRADA = '.wav'\n",
        "EXTENCION_SALIDA = '.aiff'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "ruta_audio_entrada = AUDIO_FILES_PATH + FILENAME + EXTENCION_ENTRADA\n",
        "ruta_audio_entrada_convertido = AUDIO_FILES_PATH + 'waw_conv_' + FILENAME + EXTENCION_SALIDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_consultas_df = ['id_consulta', 'id_cliente', 'formato_consulta', 'transcripción_audio', 'entrada_texto', 'productos_detectados']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "consultas_df = pd.DataFrame(columns=columns_consultas_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5W7StwLecqQV"
      },
      "outputs": [],
      "source": [
        "def record_audio(audio_file_path = AUDIO_FILES_PATH, filename = FILENAME, duration = DURATION, sample_rate = SAMPLE_RATE):\n",
        "    print(\"Grabando...\")\n",
        "    # Grabar audio con 1 canal (mono)\n",
        "    recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
        "    sd.wait()  # Esperar a que termine la grabación\n",
        "    print(\"Grabación finalizada\")\n",
        "    write(ruta_audio_entrada, sample_rate, recording)\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_wav(input_file = ruta_audio_entrada, output_file = ruta_audio_entrada_convertido):\n",
        "    sound = AudioSegment.from_file(input_file)\n",
        "    sound.export(output_file, format=\"aiff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chatbot():\n",
        "    record_audio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grabando...\n",
            "Grabación finalizada\n"
          ]
        }
      ],
      "source": [
        "chatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uso de la función convert_to_wav\n",
        "convert_to_wav(ruta_audio_entrada, ruta_audio_entrada_convertido)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PxS_YcrlCZ"
      },
      "source": [
        "## **Reconocimiento del Habla (ASR - Automatic Speech Recognition):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "YfbVSNU0m3N4"
      },
      "outputs": [],
      "source": [
        "# Crear un objeto Recognizer\n",
        "recognizer = sr.Recognizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recognize_speech():\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    try:\n",
        "        with sr.AudioFile(ruta_audio_entrada_convertido) as source:\n",
        "            audio_data = recognizer.record(source)\n",
        "            text = recognizer.recognize_sphinx(audio_data, language=\"es-ES\")\n",
        "            print(\"Texto reconocido:\", text)\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"No se pudo entender el audio\")\n",
        "    except sr.RequestError as e:\n",
        "        print(\"Error al solicitar resultados de reconocimiento de voz; {0}\".format(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error al solicitar resultados de reconocimiento de voz; missing PocketSphinx language model parameters directory: \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/pocketsphinx-data/es-ES/acoustic-model\"\n"
          ]
        }
      ],
      "source": [
        "# Ejecutar la función para reconocer el discurso\n",
        "recognize_speech()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "xEwSWIZcp4zp"
      },
      "outputs": [],
      "source": [
        "# Abrir el archivo de audio\n",
        "def transcribe_audio(audio_lang = 'es-ES'):\n",
        "    \n",
        "    with sr.AudioFile(ruta_audio_entrada_convertido) as source:\n",
        "        # Escuchar el audio (en inglés)\n",
        "        audio = recognizer.record(source)\n",
        "\n",
        "        # Utilizar Google Speech Recognition para transcribir el audio\n",
        "        try:\n",
        "            text = recognizer.recognize_google(audio, audio_lang)\n",
        "            print(\"Transcripción: \", text)\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"No se pudo entender el audio\")\n",
        "        except sr.RequestError as e:\n",
        "            print(\"Error al solicitar resultados del servicio Google Speech Recognition; {0}\".format(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "sK32Iyq1Y7QR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error al solicitar resultados del servicio Google Speech Recognition; recognition request failed: Forbidden\n"
          ]
        }
      ],
      "source": [
        "transcribe_audio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxpKQFiNm7m-"
      },
      "outputs": [],
      "source": [
        "# Transcribir y mostrar el resultado\n",
        "text = transcribe_audio(audio_file_path, audio_lang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcUSQzM7qsxY"
      },
      "source": [
        "## **2. Procesamiento del Lenguaje Natural (NLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDaWq5syqu6m"
      },
      "source": [
        "### **Preprocesamiento de Texto:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLfTLCw2q8IM"
      },
      "source": [
        "\n",
        "**Normalización del texto (eliminación de ruido, corrección ortográfica, etc.)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Br9PyerAkd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrKl19tGq9KJ"
      },
      "source": [
        "**Tokenización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5IX1gghrBXD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVbV-FF5q-af"
      },
      "source": [
        "**Eliminación de stop words.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY1ASrUnrB3I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt4d5DQcq_xW"
      },
      "source": [
        "**Lematización y stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkAeKi67rCfu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvYH9dq1GY"
      },
      "source": [
        "### **Análisis de Texto:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSCEB3hpr2y6"
      },
      "source": [
        "Análisis de Sentimiento: Determinar la emoción o tono del texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtrOgErkr4PD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzhTpz0qr4lW"
      },
      "source": [
        "Detección de Intenciones (Intent Detection): Identificar la intención del usuario utilizando modelos como BERT, GPT, RASA, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEYOkzT7r8Pj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6hMGhzr8mg"
      },
      "source": [
        "Extracción de Entidades (Entity Recognition): Extraer información relevante del texto, como nombres, fechas, ubicaciones, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgG3V-bCq2Ii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ibsChXBsNdX"
      },
      "source": [
        "## **3. Gestión del Diálogo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEnB65VssQvq"
      },
      "source": [
        "### Módulo de Gestión de Estado:\n",
        "Llevar un registro del contexto y estado del diálogo para mantener conversaciones coherentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ECriCX3sVWu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BByh2AXGsVmJ"
      },
      "source": [
        "### Motor de Respuesta:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15ewWSasb4F"
      },
      "source": [
        "Generación de Respuestas: Utilizar modelos generativos (como GPT-3) o respuestas predefinidas según las intenciones y entidades detectadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjnjzK3gseaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgvv-ETKsenO"
      },
      "source": [
        "Selección de Respuestas: Elegir la mejor respuesta entre varias opciones generadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t1AFBk2sf1P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QntMxU6_shpm"
      },
      "source": [
        "### Personalización y Contexto: Adaptar las respuestas en función del historial del usuario y el contexto actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU_3as2dsi2S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-sSu2b0soDe"
      },
      "source": [
        "## 4. Salida de Usuario (Text-to-Speech)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5eR9QLTssCK"
      },
      "source": [
        "### Conversión de Texto a Voz (TTS): Utilizar servicios como Google Text-to-Speech, Amazon Polly, o frameworks como Tacotron para convertir el texto generado en voz.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzi_CA2tswVF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3TVK99Wswe8"
      },
      "source": [
        "### Reproducción de Audio: Entregar la respuesta de voz al usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOKovFuVsyaD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
