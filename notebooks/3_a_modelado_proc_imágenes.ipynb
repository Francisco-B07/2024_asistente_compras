{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E9QcgCktnMB"
      },
      "source": [
        "##Fase 1: Comprensi칩n del Negocio\n",
        "En esta fase, se define el objetivo del proyecto y se obtiene un entendimiento del problema que se va a resolver.\n",
        "\n",
        "Objetivo del proyecto: Desarrollar un asistente de compras que pueda clasificar im치genes de frutas y verduras y encontrar el supermercado m치s barato para el producto clasificado."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fase 2: Comprensi칩n de los Datos\n",
        "En esta fase, se recopilan y exploran los datos necesarios para el proyecto."
      ],
      "metadata": {
        "id": "z6Tp2Zro7WIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FFYmdphvtl_0",
        "outputId": "24b2e44f-bf91-4c36-a5f9-61cd13ccbb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalamos las dependencias necesarias.\n",
        "!pip install tensorflow keras numpy matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavDC8oc2qK9"
      },
      "outputs": [],
      "source": [
        "# 2. Importamos las bibliotecas necesarias.\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbm4E8GU2DN7",
        "outputId": "ccf67f9d-7156-4527-ec26-7b8958fe1b6b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 3. Montamos el driver con los archivos necesarios.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0U_WmdJ-gAul",
        "outputId": "5f13a2a1-f01a-45d2-8e22-083ce51ed7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/Carrefour/\n",
            "   creating: dataset/Carrefour/Acelga/\n",
            "  inflating: dataset/Carrefour/Acelga/Acelga.jpg  \n",
            "   creating: dataset/Carrefour/Ajo/\n",
            "  inflating: dataset/Carrefour/Ajo/Ajo.jpg  \n",
            "   creating: dataset/Carrefour/Aj칤/\n",
            "  inflating: dataset/Carrefour/Aj칤/Aj칤.jpg  \n",
            "   creating: dataset/Carrefour/Albahaca/\n",
            "  inflating: dataset/Carrefour/Albahaca/Albahaca.jpg  \n",
            "   creating: dataset/Carrefour/Almendras/\n",
            "  inflating: dataset/Carrefour/Almendras/Almendras.jpg  \n",
            "   creating: dataset/Carrefour/Batata/\n",
            "  inflating: dataset/Carrefour/Batata/Batata.jpg  \n",
            "   creating: dataset/Carrefour/Berenjena/\n",
            "  inflating: dataset/Carrefour/Berenjena/Berenjena.jpg  \n",
            "   creating: dataset/Carrefour/Casta침as/\n",
            "  inflating: dataset/Carrefour/Casta침as/Casta침as.jpg  \n",
            "   creating: dataset/Carrefour/Cebolla/\n",
            "  inflating: dataset/Carrefour/Cebolla/Cebolla.jpg  \n",
            "   creating: dataset/Carrefour/Champignones/\n",
            "  inflating: dataset/Carrefour/Champignones/Champignones.jpg  \n",
            "   creating: dataset/Carrefour/Chaucha/\n",
            "  inflating: dataset/Carrefour/Chaucha/Chaucha.jpg  \n",
            "   creating: dataset/Carrefour/Chip/\n",
            "  inflating: dataset/Carrefour/Chip/Chip.jpg  \n",
            "   creating: dataset/Carrefour/Ciruelas/\n",
            "  inflating: dataset/Carrefour/Ciruelas/Ciruelas.jpg  \n",
            "   creating: dataset/Carrefour/Coco/\n",
            "  inflating: dataset/Carrefour/Coco/Coco.jpg  \n",
            "   creating: dataset/Carrefour/Ensalada/\n",
            "  inflating: dataset/Carrefour/Ensalada/Ensalada.jpg  \n",
            "   creating: dataset/Carrefour/Frutos/\n",
            "  inflating: dataset/Carrefour/Frutos/Frutos.jpg  \n",
            "   creating: dataset/Carrefour/Hongos/\n",
            "  inflating: dataset/Carrefour/Hongos/Hongos.jpg  \n",
            "   creating: dataset/Carrefour/Kale/\n",
            "  inflating: dataset/Carrefour/Kale/Kale.jpg  \n",
            "   creating: dataset/Carrefour/Lechuga/\n",
            "  inflating: dataset/Carrefour/Lechuga/Lechuga.jpg  \n",
            "   creating: dataset/Carrefour/Lima/\n",
            "  inflating: dataset/Carrefour/Lima/Lima.jpg  \n",
            "   creating: dataset/Carrefour/Limon/\n",
            "  inflating: dataset/Carrefour/Limon/Limon.jpg  \n",
            "   creating: dataset/Carrefour/Mandioca/\n",
            "  inflating: dataset/Carrefour/Mandioca/Mandioca.jpg  \n",
            "   creating: dataset/Carrefour/Man칤/\n",
            "  inflating: dataset/Carrefour/Man칤/Man칤.jpg  \n",
            "   creating: dataset/Carrefour/Mix/\n",
            "  inflating: dataset/Carrefour/Mix/Mix.jpg  \n",
            "   creating: dataset/Carrefour/Naranja/\n",
            "  inflating: dataset/Carrefour/Naranja/Naranja.jpg  \n",
            "   creating: dataset/Carrefour/Nueces/\n",
            "  inflating: dataset/Carrefour/Nueces/Nueces.jpg  \n",
            "   creating: dataset/Carrefour/Palta/\n",
            "  inflating: dataset/Carrefour/Palta/Palta.jpg  \n",
            "   creating: dataset/Carrefour/Pasas/\n",
            "  inflating: dataset/Carrefour/Pasas/Pasas.jpg  \n",
            "   creating: dataset/Carrefour/Pera/\n",
            "  inflating: dataset/Carrefour/Pera/Pera.jpg  \n",
            "   creating: dataset/Carrefour/Pi침a/\n",
            "  inflating: dataset/Carrefour/Pi침a/Pi침a.jpg  \n",
            "   creating: dataset/Carrefour/Pl치tano/\n",
            "  inflating: dataset/Carrefour/Pl치tano/Pl치tano.jpg  \n",
            "   creating: dataset/Carrefour/Repollo/\n",
            "  inflating: dataset/Carrefour/Repollo/Repollo.jpg  \n",
            "   creating: dataset/Carrefour/R칰cula/\n",
            "  inflating: dataset/Carrefour/R칰cula/R칰cula.jpg  \n",
            "   creating: dataset/Carrefour/Sand칤a/\n",
            "  inflating: dataset/Carrefour/Sand칤a/Sand칤a.jpg  \n",
            "   creating: dataset/Carrefour/Snack/\n",
            "  inflating: dataset/Carrefour/Snack/Snack.jpg  \n",
            "   creating: dataset/Carrefour/Tomate/\n",
            "  inflating: dataset/Carrefour/Tomate/Tomate.jpg  \n",
            "   creating: dataset/Carrefour/Tomates/\n",
            "  inflating: dataset/Carrefour/Tomates/Tomates.jpg  \n",
            "   creating: dataset/Jumbo/\n",
            "   creating: dataset/Jumbo/Adobo/\n",
            "  inflating: dataset/Jumbo/Adobo/Adobo.jpg  \n",
            "   creating: dataset/Jumbo/Aji/\n",
            "  inflating: dataset/Jumbo/Aji/Aji.jpg  \n",
            "   creating: dataset/Jumbo/Ajo/\n",
            "  inflating: dataset/Jumbo/Ajo/Ajo.jpg  \n",
            "   creating: dataset/Jumbo/Aj칤/\n",
            "  inflating: dataset/Jumbo/Aj칤/Aj칤.jpg  \n",
            "   creating: dataset/Jumbo/Almendras/\n",
            "  inflating: dataset/Jumbo/Almendras/Almendras.jpg  \n",
            "   creating: dataset/Jumbo/Arroz/\n",
            "  inflating: dataset/Jumbo/Arroz/Arroz.jpg  \n",
            "   creating: dataset/Jumbo/Banana/\n",
            "  inflating: dataset/Jumbo/Banana/Banana.jpg  \n",
            "   creating: dataset/Jumbo/Casta침as/\n",
            "  inflating: dataset/Jumbo/Casta침as/Casta침as.jpg  \n",
            "   creating: dataset/Jumbo/Cebolla/\n",
            "  inflating: dataset/Jumbo/Cebolla/Cebolla.jpg  \n",
            "   creating: dataset/Jumbo/Chimichurri/\n",
            "  inflating: dataset/Jumbo/Chimichurri/Chimichurri.jpg  \n",
            "   creating: dataset/Jumbo/Condimento/\n",
            "  inflating: dataset/Jumbo/Condimento/Condimento.jpg  \n",
            "   creating: dataset/Jumbo/Cookies/\n",
            "  inflating: dataset/Jumbo/Cookies/Cookies.jpg  \n",
            "   creating: dataset/Jumbo/Damascos/\n",
            "  inflating: dataset/Jumbo/Damascos/Damascos.jpg  \n",
            "   creating: dataset/Jumbo/Datiles/\n",
            "  inflating: dataset/Jumbo/Datiles/Datiles.jpg  \n",
            "   creating: dataset/Jumbo/Frambuesas/\n",
            "  inflating: dataset/Jumbo/Frambuesas/Frambuesas.jpg  \n",
            "   creating: dataset/Jumbo/Frutillas/\n",
            "  inflating: dataset/Jumbo/Frutillas/Frutillas.jpg  \n",
            "   creating: dataset/Jumbo/Garbanzos/\n",
            "  inflating: dataset/Jumbo/Garbanzos/Garbanzos.jpg  \n",
            "   creating: dataset/Jumbo/Granberries/\n",
            "  inflating: dataset/Jumbo/Granberries/Granberries.jpg  \n",
            "   creating: dataset/Jumbo/Granola/\n",
            "  inflating: dataset/Jumbo/Granola/Granola.jpg  \n",
            "   creating: dataset/Jumbo/Harina/\n",
            "  inflating: dataset/Jumbo/Harina/Harina.jpg  \n",
            "   creating: dataset/Jumbo/Higos/\n",
            "  inflating: dataset/Jumbo/Higos/Higos.jpg  \n",
            "   creating: dataset/Jumbo/Huevo/\n",
            "  inflating: dataset/Jumbo/Huevo/Huevo.jpg  \n",
            "   creating: dataset/Jumbo/Huevos/\n",
            "  inflating: dataset/Jumbo/Huevos/Huevos.jpg  \n",
            "   creating: dataset/Jumbo/Kaqui/\n",
            "  inflating: dataset/Jumbo/Kaqui/Kaqui.jpg  \n",
            "   creating: dataset/Jumbo/Lentejas/\n",
            "  inflating: dataset/Jumbo/Lentejas/Lentejas.jpg  \n",
            "   creating: dataset/Jumbo/Lima/\n",
            "  inflating: dataset/Jumbo/Lima/Lima.jpg  \n",
            "   creating: dataset/Jumbo/Lim칩n/\n",
            "  inflating: dataset/Jumbo/Lim칩n/Lim칩n.jpg  \n",
            "   creating: dataset/Jumbo/Manzana/\n",
            "  inflating: dataset/Jumbo/Manzana/Manzana.jpg  \n",
            "   creating: dataset/Jumbo/Man칤/\n",
            "  inflating: dataset/Jumbo/Man칤/Man칤.jpg  \n",
            "   creating: dataset/Jumbo/Mel칩n/\n",
            "  inflating: dataset/Jumbo/Mel칩n/Mel칩n.jpg  \n",
            "   creating: dataset/Jumbo/Mezcla/\n",
            "  inflating: dataset/Jumbo/Mezcla/Mezcla.jpg  \n",
            "   creating: dataset/Jumbo/Mix/\n",
            "  inflating: dataset/Jumbo/Mix/Mix.jpg  \n",
            "   creating: dataset/Jumbo/Naranja/\n",
            "  inflating: dataset/Jumbo/Naranja/Naranja.jpg  \n",
            "   creating: dataset/Jumbo/Nueces/\n",
            "  inflating: dataset/Jumbo/Nueces/Nueces.jpg  \n",
            "   creating: dataset/Jumbo/Palta/\n",
            "  inflating: dataset/Jumbo/Palta/Palta.jpg  \n",
            "   creating: dataset/Jumbo/Papas/\n",
            "  inflating: dataset/Jumbo/Papas/Papas.jpg  \n",
            "   creating: dataset/Jumbo/Pera/\n",
            "  inflating: dataset/Jumbo/Pera/Pera.jpg  \n",
            "   creating: dataset/Jumbo/Peras/\n",
            "  inflating: dataset/Jumbo/Peras/Peras.jpg  \n",
            "   creating: dataset/Jumbo/Pimenton/\n",
            "  inflating: dataset/Jumbo/Pimenton/Pimenton.jpg  \n",
            "   creating: dataset/Jumbo/Pizzitos/\n",
            "  inflating: dataset/Jumbo/Pizzitos/Pizzitos.jpg  \n",
            "   creating: dataset/Jumbo/Pi침a/\n",
            "  inflating: dataset/Jumbo/Pi침a/Pi침a.jpg  \n",
            "   creating: dataset/Jumbo/Pl치tano/\n",
            "  inflating: dataset/Jumbo/Pl치tano/Pl치tano.jpg  \n",
            "   creating: dataset/Jumbo/Poroto/\n",
            "  inflating: dataset/Jumbo/Poroto/Poroto.jpg  \n",
            "   creating: dataset/Jumbo/Sal/\n",
            "  inflating: dataset/Jumbo/Sal/Sal.jpg  \n",
            "   creating: dataset/Jumbo/School/\n",
            "  inflating: dataset/Jumbo/School/School.jpg  \n",
            "   creating: dataset/Jumbo/Zanahoria/\n",
            "  inflating: dataset/Jumbo/Zanahoria/Zanahoria.jpg  \n",
            "   creating: dataset/La_anonima/\n",
            "   creating: dataset/La_anonima/Acelga/\n",
            "  inflating: dataset/La_anonima/Acelga/Acelga.jpg  \n",
            "   creating: dataset/La_anonima/Achicoria/\n",
            "  inflating: dataset/La_anonima/Achicoria/Achicoria.jpg  \n",
            "   creating: dataset/La_anonima/Ajo/\n",
            "  inflating: dataset/La_anonima/Ajo/Ajo.jpg  \n",
            "   creating: dataset/La_anonima/Albahaca/\n",
            "  inflating: dataset/La_anonima/Albahaca/Albahaca.jpg  \n",
            "   creating: dataset/La_anonima/Alcaucil/\n",
            "  inflating: dataset/La_anonima/Alcaucil/Alcaucil.jpg  \n",
            "   creating: dataset/La_anonima/Almendras/\n",
            "  inflating: dataset/La_anonima/Almendras/Almendras.jpg  \n",
            "   creating: dataset/La_anonima/Ar치ndanos/\n",
            "  inflating: dataset/La_anonima/Ar치ndanos/Ar치ndanos.jpg  \n",
            "   creating: dataset/La_anonima/Banana/\n",
            "  inflating: dataset/La_anonima/Banana/Banana.jpg  \n",
            "   creating: dataset/La_anonima/Batata/\n",
            "  inflating: dataset/La_anonima/Batata/Batata.jpg  \n",
            "   creating: dataset/La_anonima/Berenjena/\n",
            "  inflating: dataset/La_anonima/Berenjena/Berenjena.jpg  \n",
            "   creating: dataset/La_anonima/Blanco/\n",
            "  inflating: dataset/La_anonima/Blanco/Blanco.jpg  \n",
            "   creating: dataset/La_anonima/Bocaditos/\n",
            "  inflating: dataset/La_anonima/Bocaditos/Bocaditos.jpg  \n",
            "   creating: dataset/La_anonima/Br칩coli/\n",
            "  inflating: dataset/La_anonima/Br칩coli/Br칩coli.jpg  \n",
            "   creating: dataset/La_anonima/Casta침as/\n",
            "  inflating: dataset/La_anonima/Casta침as/Casta침as.jpg  \n",
            "   creating: dataset/La_anonima/Cebolla/\n",
            "  inflating: dataset/La_anonima/Cebolla/Cebolla.jpg  \n",
            "   creating: dataset/La_anonima/Cereza/\n",
            "  inflating: dataset/La_anonima/Cereza/Cereza.jpg  \n",
            "   creating: dataset/La_anonima/Champignon/\n",
            "  inflating: dataset/La_anonima/Champignon/Champignon.jpg  \n",
            "   creating: dataset/La_anonima/Chips/\n",
            "  inflating: dataset/La_anonima/Chips/Chips.jpg  \n",
            "   creating: dataset/La_anonima/Choclo/\n",
            "  inflating: dataset/La_anonima/Choclo/Choclo.jpg  \n",
            "   creating: dataset/La_anonima/Ciboulette/\n",
            "  inflating: dataset/La_anonima/Ciboulette/Ciboulette.jpg  \n",
            "   creating: dataset/La_anonima/Cilantro/\n",
            "  inflating: dataset/La_anonima/Cilantro/Cilantro.jpg  \n",
            "   creating: dataset/La_anonima/Ciruela/\n",
            "  inflating: dataset/La_anonima/Ciruela/Ciruela.jpg  \n",
            "   creating: dataset/La_anonima/Ciruelas/\n",
            "  inflating: dataset/La_anonima/Ciruelas/Ciruelas.jpg  \n",
            "   creating: dataset/La_anonima/Coco/\n",
            "  inflating: dataset/La_anonima/Coco/Coco.jpg  \n",
            "   creating: dataset/La_anonima/Coliflor/\n",
            "  inflating: dataset/La_anonima/Coliflor/Coliflor.jpg  \n",
            "   creating: dataset/La_anonima/Cramberries/\n",
            "  inflating: dataset/La_anonima/Cramberries/Cramberries.jpg  \n",
            "   creating: dataset/La_anonima/Durazno/\n",
            "  inflating: dataset/La_anonima/Durazno/Durazno.jpg  \n",
            "   creating: dataset/La_anonima/Ensalada/\n",
            "  inflating: dataset/La_anonima/Ensalada/Ensalada.jpg  \n",
            "   creating: dataset/La_anonima/Espinaca/\n",
            "  inflating: dataset/La_anonima/Espinaca/Espinaca.jpg  \n",
            "   creating: dataset/La_anonima/Frutas/\n",
            "  inflating: dataset/La_anonima/Frutas/Frutas.jpg  \n",
            "   creating: dataset/La_anonima/Garrapi침ada/\n",
            "  inflating: dataset/La_anonima/Garrapi침ada/Garrapi침ada.jpg  \n",
            "   creating: dataset/La_anonima/Girasol/\n",
            "  inflating: dataset/La_anonima/Girasol/Girasol.jpg  \n",
            "   creating: dataset/La_anonima/Higos/\n",
            "  inflating: dataset/La_anonima/Higos/Higos.jpg  \n",
            "   creating: dataset/La_anonima/Hinojo/\n",
            "  inflating: dataset/La_anonima/Hinojo/Hinojo.jpg  \n",
            "   creating: dataset/La_anonima/Hongos/\n",
            "  inflating: dataset/La_anonima/Hongos/Hongos.jpg  \n",
            "   creating: dataset/La_anonima/Jengibre/\n",
            "  inflating: dataset/La_anonima/Jengibre/Jengibre.jpg  \n",
            "   creating: dataset/La_anonima/Kale/\n",
            "  inflating: dataset/La_anonima/Kale/Kale.jpg  \n",
            "   creating: dataset/La_anonima/Kiwi/\n",
            "  inflating: dataset/La_anonima/Kiwi/Kiwi.jpg  \n",
            "   creating: dataset/La_anonima/Lechuga/\n",
            "  inflating: dataset/La_anonima/Lechuga/Lechuga.jpg  \n",
            "   creating: dataset/La_anonima/Lim칩n/\n",
            "  inflating: dataset/La_anonima/Lim칩n/Lim칩n.jpg  \n",
            "   creating: dataset/La_anonima/Mandarina/\n",
            "  inflating: dataset/La_anonima/Mandarina/Mandarina.jpg  \n",
            "   creating: dataset/La_anonima/Mango/\n",
            "  inflating: dataset/La_anonima/Mango/Mango.jpg  \n",
            "   creating: dataset/La_anonima/Manzana/\n",
            "  inflating: dataset/La_anonima/Manzana/Manzana.jpg  \n",
            "   creating: dataset/La_anonima/Man칤/\n",
            "  inflating: dataset/La_anonima/Man칤/Man칤.jpg  \n",
            "   creating: dataset/La_anonima/Ma칤z/\n",
            "  inflating: dataset/La_anonima/Ma칤z/Ma칤z.jpg  \n",
            "   creating: dataset/La_anonima/Mel칩n/\n",
            "  inflating: dataset/La_anonima/Mel칩n/Mel칩n.jpg  \n",
            "   creating: dataset/La_anonima/Mix/\n",
            "  inflating: dataset/La_anonima/Mix/Mix.jpg  \n",
            "   creating: dataset/La_anonima/Morr칩n/\n",
            "  inflating: dataset/La_anonima/Morr칩n/Morr칩n.jpg  \n",
            "   creating: dataset/La_anonima/Naranja/\n",
            "  inflating: dataset/La_anonima/Naranja/Naranja.jpg  \n",
            "   creating: dataset/La_anonima/Nueces/\n",
            "  inflating: dataset/La_anonima/Nueces/Nueces.jpg  \n",
            "   creating: dataset/La_anonima/Nuez/\n",
            "  inflating: dataset/La_anonima/Nuez/Nuez.jpg  \n",
            "   creating: dataset/La_anonima/Papa/\n",
            "  inflating: dataset/La_anonima/Papa/Papa.jpg  \n",
            "   creating: dataset/La_anonima/Papines/\n",
            "  inflating: dataset/La_anonima/Papines/Papines.jpg  \n",
            "   creating: dataset/La_anonima/Pasas/\n",
            "  inflating: dataset/La_anonima/Pasas/Pasas.jpg  \n",
            "   creating: dataset/La_anonima/Pel칩n/\n",
            "  inflating: dataset/La_anonima/Pel칩n/Pel칩n.jpg  \n",
            "   creating: dataset/La_anonima/Pepino/\n",
            "  inflating: dataset/La_anonima/Pepino/Pepino.jpg  \n",
            "   creating: dataset/La_anonima/Pera/\n",
            "  inflating: dataset/La_anonima/Pera/Pera.jpg  \n",
            "   creating: dataset/La_anonima/Perejil/\n",
            "  inflating: dataset/La_anonima/Perejil/Perejil.jpg  \n",
            "   creating: dataset/La_anonima/Pimienta/\n",
            "  inflating: dataset/La_anonima/Pimienta/Pimienta.jpg  \n",
            "   creating: dataset/La_anonima/Pipas/\n",
            "  inflating: dataset/La_anonima/Pipas/Pipas.jpg  \n",
            "   creating: dataset/La_anonima/Pistacho/\n",
            "  inflating: dataset/La_anonima/Pistacho/Pistacho.jpg  \n",
            "   creating: dataset/La_anonima/Pistachos/\n",
            "  inflating: dataset/La_anonima/Pistachos/Pistachos.jpg  \n",
            "   creating: dataset/La_anonima/Pi침a/\n",
            "  inflating: dataset/La_anonima/Pi침a/Pi침a.jpg  \n",
            "   creating: dataset/La_anonima/Pl치tano/\n",
            "  inflating: dataset/La_anonima/Pl치tano/Pl치tano.jpg  \n",
            "   creating: dataset/La_anonima/Pomelo/\n",
            "  inflating: dataset/La_anonima/Pomelo/Pomelo.jpg  \n",
            "   creating: dataset/La_anonima/Quinoa/\n",
            "  inflating: dataset/La_anonima/Quinoa/Quinoa.jpg  \n",
            "   creating: dataset/La_anonima/Rabanito/\n",
            "  inflating: dataset/La_anonima/Rabanito/Rabanito.jpg  \n",
            "   creating: dataset/La_anonima/Radiccio/\n",
            "  inflating: dataset/La_anonima/Radiccio/Radiccio.jpg  \n",
            "   creating: dataset/La_anonima/Radicheta/\n",
            "  inflating: dataset/La_anonima/Radicheta/Radicheta.jpg  \n",
            "   creating: dataset/La_anonima/Remolacha/\n",
            "  inflating: dataset/La_anonima/Remolacha/Remolacha.jpg  \n",
            "   creating: dataset/La_anonima/Repollo/\n",
            "  inflating: dataset/La_anonima/Repollo/Repollo.jpg  \n",
            "   creating: dataset/La_anonima/Ristra/\n",
            "  inflating: dataset/La_anonima/Ristra/Ristra.jpg  \n",
            "   creating: dataset/La_anonima/R칰cula/\n",
            "  inflating: dataset/La_anonima/R칰cula/R칰cula.jpg  \n",
            "   creating: dataset/La_anonima/Sand칤a/\n",
            "  inflating: dataset/La_anonima/Sand칤a/Sand칤a.jpg  \n",
            "   creating: dataset/La_anonima/Semilla/\n",
            "  inflating: dataset/La_anonima/Semilla/Semilla.jpg  \n",
            "   creating: dataset/La_anonima/Semillas/\n",
            "  inflating: dataset/La_anonima/Semillas/Semillas.jpg  \n",
            "   creating: dataset/La_anonima/Soja/\n",
            "  inflating: dataset/La_anonima/Soja/Soja.jpg  \n",
            "   creating: dataset/La_anonima/S칠samo/\n",
            "  inflating: dataset/La_anonima/S칠samo/S칠samo.jpg  \n",
            "   creating: dataset/La_anonima/Tomate/\n",
            "  inflating: dataset/La_anonima/Tomate/Tomate.jpg  \n",
            "   creating: dataset/La_anonima/Tomates/\n",
            "  inflating: dataset/La_anonima/Tomates/Tomates.jpg  \n",
            "   creating: dataset/La_anonima/Uvas/\n",
            "  inflating: dataset/La_anonima/Uvas/Uvas.jpg  \n",
            "   creating: dataset/La_anonima/Vegetales/\n",
            "  inflating: dataset/La_anonima/Vegetales/Vegetales.jpg  \n",
            "   creating: dataset/La_anonima/Zanahoria/\n",
            "  inflating: dataset/La_anonima/Zanahoria/Zanahoria.jpg  \n",
            "   creating: dataset/La_anonima/Zapallito/\n",
            "  inflating: dataset/La_anonima/Zapallito/Zapallito.jpg  \n",
            "   creating: dataset/La_anonima/Zapallo/\n",
            "  inflating: dataset/La_anonima/Zapallo/Zapallo.jpg  \n",
            "   creating: dataset/scripts/\n",
            "  inflating: dataset/scripts/normalizar_datos.py  \n",
            "  inflating: dataset/scripts/normalizar_imagenes.py  \n",
            "  inflating: dataset/scripts/script_carpetas.py  \n",
            "  inflating: dataset/VerdurasNormalizadas.csv  \n"
          ]
        }
      ],
      "source": [
        "# 4. Descomprimimos el archivo con el dataset.\n",
        "!unzip /content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS_wvmGByRim"
      },
      "source": [
        "##Fase 3: Preparaci칩n de los Datos\n",
        "En esta fase, se preparan los datos para su an치lisis y modelado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Cargar y preprocesar las im치genes.\n",
        "# Directorio base donde se encuentran las im치genes originales\n",
        "data_dir = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset'\n",
        "# Directorio base para las im치genes aumentadas\n",
        "augmented_data_dir = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset/augmented_data'\n",
        "\n",
        "# Configuraci칩n de la aumentaci칩n de datos\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Recorrer cada supermercado y cada fruta/verdura para generar im치genes aumentadas\n",
        "for supermarket in os.listdir(data_dir):\n",
        "    supermarket_path = os.path.join(data_dir, supermarket)\n",
        "    if os.path.isdir(supermarket_path):\n",
        "        for fruit in os.listdir(supermarket_path):\n",
        "            fruit_path = os.path.join(supermarket_path, fruit)\n",
        "            if os.path.isdir(fruit_path):\n",
        "                # Crear directorio para las im치genes aumentadas si no existe\n",
        "                save_to_dir = os.path.join(augmented_data_dir, supermarket, fruit)\n",
        "                os.makedirs(save_to_dir, exist_ok=True) # Aseg칰rate de que el directorio existe\n",
        "                for img_name in os.listdir(fruit_path):\n",
        "                    img_path = os.path.join(fruit_path, img_name)\n",
        "                    try:\n",
        "                        img = tf.keras.preprocessing.image.load_img(img_path)\n",
        "                        x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "                        x = x.reshape((1,) + x.shape)\n",
        "                        # Generar im치genes aumentadas\n",
        "                        i = 0\n",
        "                        for batch in datagen.flow(x, batch_size=1, save_to_dir=save_to_dir, save_prefix=fruit, save_format='jpeg'):\n",
        "                            i += 1\n",
        "                            if i > 50: # Generar 50 im치genes nuevas por imagen original\n",
        "                                break\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error procesando la imagen {img_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "q1BasVgV7zGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio base actual donde se encuentran las im치genes aumentadas\n",
        "current_augmented_data_dir = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset/augmented_data'\n",
        "\n",
        "# Nuevo directorio base donde se reorganizar치n las im치genes aumentadas\n",
        "new_augmented_data_dir = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset/reorganized_data'\n",
        "os.makedirs(new_augmented_data_dir, exist_ok=True)  # Crear el directorio si no existe\n",
        "\n",
        "# Recorrer cada supermercado y cada fruta/verdura en el directorio actual\n",
        "for supermarket in os.listdir(current_augmented_data_dir):\n",
        "    supermarket_path = os.path.join(current_augmented_data_dir, supermarket)\n",
        "    if os.path.isdir(supermarket_path):\n",
        "        for fruit in os.listdir(supermarket_path):\n",
        "            fruit_path = os.path.join(supermarket_path, fruit)\n",
        "            if os.path.isdir(fruit_path):\n",
        "                for img_name in os.listdir(fruit_path):\n",
        "                    img_path = os.path.join(fruit_path, img_name)\n",
        "                    try:\n",
        "                        # Crear la nueva ruta para la imagen\n",
        "                        new_dir_path = os.path.join(new_augmented_data_dir, fruit, supermarket)\n",
        "                        os.makedirs(new_dir_path, exist_ok=True)  # Crear el directorio si no existe\n",
        "\n",
        "                        # Mover la imagen a la nueva ubicaci칩n\n",
        "                        new_img_path = os.path.join(new_dir_path, img_name)\n",
        "                        shutil.move(img_path, new_img_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error moviendo la imagen {img_path}: {e}\")\n",
        "\n",
        "print(\"Reorganizaci칩n completa.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpKe5J3WFcCY",
        "outputId": "1ed299f5-e18a-468a-e951-66eb83edac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reorganizaci칩n completa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kaLsR7bukLF"
      },
      "source": [
        "##Fase 4: Modelado\n",
        "En esta fase, se crean y entrenan los modelos de machine learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsJq5R3FZc2r"
      },
      "outputs": [],
      "source": [
        "# 6. Entrenamos y guardamos el modelo.\n",
        "# Directorios de datos\n",
        "base_dir = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/reorganized_dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Par치metros\n",
        "img_height, img_width = 150, 150\n",
        "batch_size = 32\n",
        "epochs = 75\n",
        "\n",
        "# Generadores de datos con aumentaci칩n\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2 # Utilizamos el 20% de los datos para validaci칩n\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Definir el modelo\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Guardar json con la clase de los datos\n",
        "with open('class_indices.json', 'w') as f:\n",
        "    json.dump(train_generator.class_indices, f)\n",
        "\n",
        "# Guardar el modelo final\n",
        "model.save('final_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fase 5: Evaluaci칩n\n",
        "En esta fase, se eval칰a el modelo entrenado para asegurarse de que cumple con los requisitos del negocio."
      ],
      "metadata": {
        "id": "FuF6DtZginWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36qCIRGUExMw"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo guardado\n",
        "model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Model accuracy: {accuracy*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizaci칩n con Matplotlib\n",
        "def plot_training_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Llamar a la funci칩n para plotear la historia del entrenamiento\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "HVG4LuZ2-W11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fase 6: Despliegue\n",
        "En esta fase, se implementa el modelo en un entorno de producci칩n y se prepara para su uso.\n"
      ],
      "metadata": {
        "id": "o_5fpvOEi2yR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNt4Mei2ENrw"
      },
      "outputs": [],
      "source": [
        "# 8. Preparamos el archivo CSV para su procesamiento.\n",
        "# Ruta del archivo CSV\n",
        "prices_path = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/VerdurasNormalizadas.csv'\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "prices_df = pd.read_csv(prices_path)\n",
        "\n",
        "# Limpiar y convertir los precios a valores num칠ricos\n",
        "prices_df['Precio'] = prices_df['Precio'].str.replace(r'[^0-9]', '', regex=True)\n",
        "prices_df['Precio'] = pd.to_numeric(prices_df['Precio'], errors='coerce') / 100\n",
        "\n",
        "# Imprimir el DataFrame resultante\n",
        "print(prices_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Probamos el modelo y su funcionalidad."
      ],
      "metadata": {
        "id": "KA10zayei_LG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVnxP2L6-NqE"
      },
      "outputs": [],
      "source": [
        "# 9. Probamos el modelo y su funcionalidad.\n",
        "# Funci칩n para encontrar el supermercado m치s barato para un producto dado\n",
        "def supermercado_mas_barato(producto, prices_df):\n",
        "    precios_producto = prices_df[prices_df['Producto'] == producto]\n",
        "    supermercado_mas_barato = precios_producto.loc[precios_producto['Precio'].idxmin(), 'Supermercado']\n",
        "    precio_mas_bajo = precios_producto['Precio'].min()\n",
        "    return supermercado_mas_barato, precio_mas_bajo\n",
        "\n",
        "# Cargar el modelo previamente entrenado\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/ISPC-3춿AN팪O/asistente_compras_app/best_model.h5')\n",
        "\n",
        "# Funci칩n para clasificar una imagen y encontrar el supermercado m치s barato para esa categor칤a\n",
        "def clasificar_y_encontrar_supermercado(imagen_path, prices_df):\n",
        "    # Cargar la imagen y preprocesarla\n",
        "    img = load_img(imagen_path, target_size=(150, 150))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array.reshape((1,) + img_array.shape)\n",
        "    img_array /= 255.0 # Normalizar los valores de p칤xeles\n",
        "\n",
        "    # Clasificar la imagen utilizando el modelo\n",
        "    prediction = model.predict(img_array)\n",
        "    clase_predicha = np.argmax(prediction)\n",
        "\n",
        "    # Mapear la clase predicha al nombre de la fruta o verdura\n",
        "    if train_generator:\n",
        "        clases = train_generator.class_indices\n",
        "        frutas_verduras = {v: k for k, v in clases.items()}\n",
        "        producto = frutas_verduras[clase_predicha]\n",
        "\n",
        "        # Encontrar el supermercado m치s barato para el producto dado\n",
        "        supermercado, precio = supermercado_mas_barato(producto, prices_df)\n",
        "        return producto, supermercado, precio\n",
        "    else:\n",
        "        return None, None, None\n",
        "\n",
        "# Ruta de la imagen que deseas clasificar y comparar precios\n",
        "imagen_path = '/content/drive/MyDrive/ISPC-3춿AN팪O/AsistenteDeCompras/dataset/Carrefour/Coco/Coco.jpg'\n",
        "\n",
        "# Clasificar la imagen y encontrar el supermercado m치s barato\n",
        "producto, supermercado, precio = clasificar_y_encontrar_supermercado(imagen_path, prices_df)\n",
        "\n",
        "# Imprimir el resultado\n",
        "if producto:\n",
        "    print(f\"La imagen es de un(a) {producto}.\")\n",
        "    print(f\"El supermercado m치s barato para {producto} es {supermercado} con un precio de ${precio:.2f}.\")\n",
        "else:\n",
        "    print(\"No se pudo clasificar la imagen.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusi칩n\n",
        "Este proyecto utiliza una combinaci칩n de t칠cnicas de aprendizaje profundo y procesamiento de im치genes para crear un asistente de compras capaz de clasificar im치genes de frutas y verduras y encontrar el supermercado m치s barato para estos productos. Las herramientas y bibliotecas de Python proporcionan un entorno robusto para desarrollar, entrenar, evaluar y desplegar el modelo, permitiendo una implementaci칩n efectiva de la soluci칩n propuesta."
      ],
      "metadata": {
        "id": "vxjlr8s99DXZ"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}