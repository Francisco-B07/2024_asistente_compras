{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E9QcgCktnMB"
      },
      "source": [
        "##Fase 1: Comprensión del Negocio\n",
        "En esta fase, se define el objetivo del proyecto y se obtiene un entendimiento del problema que se va a resolver.\n",
        "\n",
        "Objetivo del proyecto: Desarrollar un asistente de compras que pueda clasificar imágenes de frutas y verduras y encontrar el supermercado más barato para el producto clasificado."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fase 2: Comprensión de los Datos\n",
        "En esta fase, se recopilan y exploran los datos necesarios para el proyecto."
      ],
      "metadata": {
        "id": "z6Tp2Zro7WIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FFYmdphvtl_0",
        "outputId": "24b2e44f-bf91-4c36-a5f9-61cd13ccbb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalamos las dependencias necesarias.\n",
        "!pip install tensorflow keras numpy matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavDC8oc2qK9"
      },
      "outputs": [],
      "source": [
        "# 2. Importamos las bibliotecas necesarias.\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbm4E8GU2DN7",
        "outputId": "ccf67f9d-7156-4527-ec26-7b8958fe1b6b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 3. Montamos el driver con los archivos necesarios.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0U_WmdJ-gAul",
        "outputId": "5f13a2a1-f01a-45d2-8e22-083ce51ed7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/Carrefour/\n",
            "   creating: dataset/Carrefour/Acelga/\n",
            "  inflating: dataset/Carrefour/Acelga/Acelga.jpg  \n",
            "   creating: dataset/Carrefour/Ajo/\n",
            "  inflating: dataset/Carrefour/Ajo/Ajo.jpg  \n",
            "   creating: dataset/Carrefour/Ají/\n",
            "  inflating: dataset/Carrefour/Ají/Ají.jpg  \n",
            "   creating: dataset/Carrefour/Albahaca/\n",
            "  inflating: dataset/Carrefour/Albahaca/Albahaca.jpg  \n",
            "   creating: dataset/Carrefour/Almendras/\n",
            "  inflating: dataset/Carrefour/Almendras/Almendras.jpg  \n",
            "   creating: dataset/Carrefour/Batata/\n",
            "  inflating: dataset/Carrefour/Batata/Batata.jpg  \n",
            "   creating: dataset/Carrefour/Berenjena/\n",
            "  inflating: dataset/Carrefour/Berenjena/Berenjena.jpg  \n",
            "   creating: dataset/Carrefour/Castañas/\n",
            "  inflating: dataset/Carrefour/Castañas/Castañas.jpg  \n",
            "   creating: dataset/Carrefour/Cebolla/\n",
            "  inflating: dataset/Carrefour/Cebolla/Cebolla.jpg  \n",
            "   creating: dataset/Carrefour/Champignones/\n",
            "  inflating: dataset/Carrefour/Champignones/Champignones.jpg  \n",
            "   creating: dataset/Carrefour/Chaucha/\n",
            "  inflating: dataset/Carrefour/Chaucha/Chaucha.jpg  \n",
            "   creating: dataset/Carrefour/Chip/\n",
            "  inflating: dataset/Carrefour/Chip/Chip.jpg  \n",
            "   creating: dataset/Carrefour/Ciruelas/\n",
            "  inflating: dataset/Carrefour/Ciruelas/Ciruelas.jpg  \n",
            "   creating: dataset/Carrefour/Coco/\n",
            "  inflating: dataset/Carrefour/Coco/Coco.jpg  \n",
            "   creating: dataset/Carrefour/Ensalada/\n",
            "  inflating: dataset/Carrefour/Ensalada/Ensalada.jpg  \n",
            "   creating: dataset/Carrefour/Frutos/\n",
            "  inflating: dataset/Carrefour/Frutos/Frutos.jpg  \n",
            "   creating: dataset/Carrefour/Hongos/\n",
            "  inflating: dataset/Carrefour/Hongos/Hongos.jpg  \n",
            "   creating: dataset/Carrefour/Kale/\n",
            "  inflating: dataset/Carrefour/Kale/Kale.jpg  \n",
            "   creating: dataset/Carrefour/Lechuga/\n",
            "  inflating: dataset/Carrefour/Lechuga/Lechuga.jpg  \n",
            "   creating: dataset/Carrefour/Lima/\n",
            "  inflating: dataset/Carrefour/Lima/Lima.jpg  \n",
            "   creating: dataset/Carrefour/Limon/\n",
            "  inflating: dataset/Carrefour/Limon/Limon.jpg  \n",
            "   creating: dataset/Carrefour/Mandioca/\n",
            "  inflating: dataset/Carrefour/Mandioca/Mandioca.jpg  \n",
            "   creating: dataset/Carrefour/Maní/\n",
            "  inflating: dataset/Carrefour/Maní/Maní.jpg  \n",
            "   creating: dataset/Carrefour/Mix/\n",
            "  inflating: dataset/Carrefour/Mix/Mix.jpg  \n",
            "   creating: dataset/Carrefour/Naranja/\n",
            "  inflating: dataset/Carrefour/Naranja/Naranja.jpg  \n",
            "   creating: dataset/Carrefour/Nueces/\n",
            "  inflating: dataset/Carrefour/Nueces/Nueces.jpg  \n",
            "   creating: dataset/Carrefour/Palta/\n",
            "  inflating: dataset/Carrefour/Palta/Palta.jpg  \n",
            "   creating: dataset/Carrefour/Pasas/\n",
            "  inflating: dataset/Carrefour/Pasas/Pasas.jpg  \n",
            "   creating: dataset/Carrefour/Pera/\n",
            "  inflating: dataset/Carrefour/Pera/Pera.jpg  \n",
            "   creating: dataset/Carrefour/Piña/\n",
            "  inflating: dataset/Carrefour/Piña/Piña.jpg  \n",
            "   creating: dataset/Carrefour/Plátano/\n",
            "  inflating: dataset/Carrefour/Plátano/Plátano.jpg  \n",
            "   creating: dataset/Carrefour/Repollo/\n",
            "  inflating: dataset/Carrefour/Repollo/Repollo.jpg  \n",
            "   creating: dataset/Carrefour/Rúcula/\n",
            "  inflating: dataset/Carrefour/Rúcula/Rúcula.jpg  \n",
            "   creating: dataset/Carrefour/Sandía/\n",
            "  inflating: dataset/Carrefour/Sandía/Sandía.jpg  \n",
            "   creating: dataset/Carrefour/Snack/\n",
            "  inflating: dataset/Carrefour/Snack/Snack.jpg  \n",
            "   creating: dataset/Carrefour/Tomate/\n",
            "  inflating: dataset/Carrefour/Tomate/Tomate.jpg  \n",
            "   creating: dataset/Carrefour/Tomates/\n",
            "  inflating: dataset/Carrefour/Tomates/Tomates.jpg  \n",
            "   creating: dataset/Jumbo/\n",
            "   creating: dataset/Jumbo/Adobo/\n",
            "  inflating: dataset/Jumbo/Adobo/Adobo.jpg  \n",
            "   creating: dataset/Jumbo/Aji/\n",
            "  inflating: dataset/Jumbo/Aji/Aji.jpg  \n",
            "   creating: dataset/Jumbo/Ajo/\n",
            "  inflating: dataset/Jumbo/Ajo/Ajo.jpg  \n",
            "   creating: dataset/Jumbo/Ají/\n",
            "  inflating: dataset/Jumbo/Ají/Ají.jpg  \n",
            "   creating: dataset/Jumbo/Almendras/\n",
            "  inflating: dataset/Jumbo/Almendras/Almendras.jpg  \n",
            "   creating: dataset/Jumbo/Arroz/\n",
            "  inflating: dataset/Jumbo/Arroz/Arroz.jpg  \n",
            "   creating: dataset/Jumbo/Banana/\n",
            "  inflating: dataset/Jumbo/Banana/Banana.jpg  \n",
            "   creating: dataset/Jumbo/Castañas/\n",
            "  inflating: dataset/Jumbo/Castañas/Castañas.jpg  \n",
            "   creating: dataset/Jumbo/Cebolla/\n",
            "  inflating: dataset/Jumbo/Cebolla/Cebolla.jpg  \n",
            "   creating: dataset/Jumbo/Chimichurri/\n",
            "  inflating: dataset/Jumbo/Chimichurri/Chimichurri.jpg  \n",
            "   creating: dataset/Jumbo/Condimento/\n",
            "  inflating: dataset/Jumbo/Condimento/Condimento.jpg  \n",
            "   creating: dataset/Jumbo/Cookies/\n",
            "  inflating: dataset/Jumbo/Cookies/Cookies.jpg  \n",
            "   creating: dataset/Jumbo/Damascos/\n",
            "  inflating: dataset/Jumbo/Damascos/Damascos.jpg  \n",
            "   creating: dataset/Jumbo/Datiles/\n",
            "  inflating: dataset/Jumbo/Datiles/Datiles.jpg  \n",
            "   creating: dataset/Jumbo/Frambuesas/\n",
            "  inflating: dataset/Jumbo/Frambuesas/Frambuesas.jpg  \n",
            "   creating: dataset/Jumbo/Frutillas/\n",
            "  inflating: dataset/Jumbo/Frutillas/Frutillas.jpg  \n",
            "   creating: dataset/Jumbo/Garbanzos/\n",
            "  inflating: dataset/Jumbo/Garbanzos/Garbanzos.jpg  \n",
            "   creating: dataset/Jumbo/Granberries/\n",
            "  inflating: dataset/Jumbo/Granberries/Granberries.jpg  \n",
            "   creating: dataset/Jumbo/Granola/\n",
            "  inflating: dataset/Jumbo/Granola/Granola.jpg  \n",
            "   creating: dataset/Jumbo/Harina/\n",
            "  inflating: dataset/Jumbo/Harina/Harina.jpg  \n",
            "   creating: dataset/Jumbo/Higos/\n",
            "  inflating: dataset/Jumbo/Higos/Higos.jpg  \n",
            "   creating: dataset/Jumbo/Huevo/\n",
            "  inflating: dataset/Jumbo/Huevo/Huevo.jpg  \n",
            "   creating: dataset/Jumbo/Huevos/\n",
            "  inflating: dataset/Jumbo/Huevos/Huevos.jpg  \n",
            "   creating: dataset/Jumbo/Kaqui/\n",
            "  inflating: dataset/Jumbo/Kaqui/Kaqui.jpg  \n",
            "   creating: dataset/Jumbo/Lentejas/\n",
            "  inflating: dataset/Jumbo/Lentejas/Lentejas.jpg  \n",
            "   creating: dataset/Jumbo/Lima/\n",
            "  inflating: dataset/Jumbo/Lima/Lima.jpg  \n",
            "   creating: dataset/Jumbo/Limón/\n",
            "  inflating: dataset/Jumbo/Limón/Limón.jpg  \n",
            "   creating: dataset/Jumbo/Manzana/\n",
            "  inflating: dataset/Jumbo/Manzana/Manzana.jpg  \n",
            "   creating: dataset/Jumbo/Maní/\n",
            "  inflating: dataset/Jumbo/Maní/Maní.jpg  \n",
            "   creating: dataset/Jumbo/Melón/\n",
            "  inflating: dataset/Jumbo/Melón/Melón.jpg  \n",
            "   creating: dataset/Jumbo/Mezcla/\n",
            "  inflating: dataset/Jumbo/Mezcla/Mezcla.jpg  \n",
            "   creating: dataset/Jumbo/Mix/\n",
            "  inflating: dataset/Jumbo/Mix/Mix.jpg  \n",
            "   creating: dataset/Jumbo/Naranja/\n",
            "  inflating: dataset/Jumbo/Naranja/Naranja.jpg  \n",
            "   creating: dataset/Jumbo/Nueces/\n",
            "  inflating: dataset/Jumbo/Nueces/Nueces.jpg  \n",
            "   creating: dataset/Jumbo/Palta/\n",
            "  inflating: dataset/Jumbo/Palta/Palta.jpg  \n",
            "   creating: dataset/Jumbo/Papas/\n",
            "  inflating: dataset/Jumbo/Papas/Papas.jpg  \n",
            "   creating: dataset/Jumbo/Pera/\n",
            "  inflating: dataset/Jumbo/Pera/Pera.jpg  \n",
            "   creating: dataset/Jumbo/Peras/\n",
            "  inflating: dataset/Jumbo/Peras/Peras.jpg  \n",
            "   creating: dataset/Jumbo/Pimenton/\n",
            "  inflating: dataset/Jumbo/Pimenton/Pimenton.jpg  \n",
            "   creating: dataset/Jumbo/Pizzitos/\n",
            "  inflating: dataset/Jumbo/Pizzitos/Pizzitos.jpg  \n",
            "   creating: dataset/Jumbo/Piña/\n",
            "  inflating: dataset/Jumbo/Piña/Piña.jpg  \n",
            "   creating: dataset/Jumbo/Plátano/\n",
            "  inflating: dataset/Jumbo/Plátano/Plátano.jpg  \n",
            "   creating: dataset/Jumbo/Poroto/\n",
            "  inflating: dataset/Jumbo/Poroto/Poroto.jpg  \n",
            "   creating: dataset/Jumbo/Sal/\n",
            "  inflating: dataset/Jumbo/Sal/Sal.jpg  \n",
            "   creating: dataset/Jumbo/School/\n",
            "  inflating: dataset/Jumbo/School/School.jpg  \n",
            "   creating: dataset/Jumbo/Zanahoria/\n",
            "  inflating: dataset/Jumbo/Zanahoria/Zanahoria.jpg  \n",
            "   creating: dataset/La_anonima/\n",
            "   creating: dataset/La_anonima/Acelga/\n",
            "  inflating: dataset/La_anonima/Acelga/Acelga.jpg  \n",
            "   creating: dataset/La_anonima/Achicoria/\n",
            "  inflating: dataset/La_anonima/Achicoria/Achicoria.jpg  \n",
            "   creating: dataset/La_anonima/Ajo/\n",
            "  inflating: dataset/La_anonima/Ajo/Ajo.jpg  \n",
            "   creating: dataset/La_anonima/Albahaca/\n",
            "  inflating: dataset/La_anonima/Albahaca/Albahaca.jpg  \n",
            "   creating: dataset/La_anonima/Alcaucil/\n",
            "  inflating: dataset/La_anonima/Alcaucil/Alcaucil.jpg  \n",
            "   creating: dataset/La_anonima/Almendras/\n",
            "  inflating: dataset/La_anonima/Almendras/Almendras.jpg  \n",
            "   creating: dataset/La_anonima/Arándanos/\n",
            "  inflating: dataset/La_anonima/Arándanos/Arándanos.jpg  \n",
            "   creating: dataset/La_anonima/Banana/\n",
            "  inflating: dataset/La_anonima/Banana/Banana.jpg  \n",
            "   creating: dataset/La_anonima/Batata/\n",
            "  inflating: dataset/La_anonima/Batata/Batata.jpg  \n",
            "   creating: dataset/La_anonima/Berenjena/\n",
            "  inflating: dataset/La_anonima/Berenjena/Berenjena.jpg  \n",
            "   creating: dataset/La_anonima/Blanco/\n",
            "  inflating: dataset/La_anonima/Blanco/Blanco.jpg  \n",
            "   creating: dataset/La_anonima/Bocaditos/\n",
            "  inflating: dataset/La_anonima/Bocaditos/Bocaditos.jpg  \n",
            "   creating: dataset/La_anonima/Brócoli/\n",
            "  inflating: dataset/La_anonima/Brócoli/Brócoli.jpg  \n",
            "   creating: dataset/La_anonima/Castañas/\n",
            "  inflating: dataset/La_anonima/Castañas/Castañas.jpg  \n",
            "   creating: dataset/La_anonima/Cebolla/\n",
            "  inflating: dataset/La_anonima/Cebolla/Cebolla.jpg  \n",
            "   creating: dataset/La_anonima/Cereza/\n",
            "  inflating: dataset/La_anonima/Cereza/Cereza.jpg  \n",
            "   creating: dataset/La_anonima/Champignon/\n",
            "  inflating: dataset/La_anonima/Champignon/Champignon.jpg  \n",
            "   creating: dataset/La_anonima/Chips/\n",
            "  inflating: dataset/La_anonima/Chips/Chips.jpg  \n",
            "   creating: dataset/La_anonima/Choclo/\n",
            "  inflating: dataset/La_anonima/Choclo/Choclo.jpg  \n",
            "   creating: dataset/La_anonima/Ciboulette/\n",
            "  inflating: dataset/La_anonima/Ciboulette/Ciboulette.jpg  \n",
            "   creating: dataset/La_anonima/Cilantro/\n",
            "  inflating: dataset/La_anonima/Cilantro/Cilantro.jpg  \n",
            "   creating: dataset/La_anonima/Ciruela/\n",
            "  inflating: dataset/La_anonima/Ciruela/Ciruela.jpg  \n",
            "   creating: dataset/La_anonima/Ciruelas/\n",
            "  inflating: dataset/La_anonima/Ciruelas/Ciruelas.jpg  \n",
            "   creating: dataset/La_anonima/Coco/\n",
            "  inflating: dataset/La_anonima/Coco/Coco.jpg  \n",
            "   creating: dataset/La_anonima/Coliflor/\n",
            "  inflating: dataset/La_anonima/Coliflor/Coliflor.jpg  \n",
            "   creating: dataset/La_anonima/Cramberries/\n",
            "  inflating: dataset/La_anonima/Cramberries/Cramberries.jpg  \n",
            "   creating: dataset/La_anonima/Durazno/\n",
            "  inflating: dataset/La_anonima/Durazno/Durazno.jpg  \n",
            "   creating: dataset/La_anonima/Ensalada/\n",
            "  inflating: dataset/La_anonima/Ensalada/Ensalada.jpg  \n",
            "   creating: dataset/La_anonima/Espinaca/\n",
            "  inflating: dataset/La_anonima/Espinaca/Espinaca.jpg  \n",
            "   creating: dataset/La_anonima/Frutas/\n",
            "  inflating: dataset/La_anonima/Frutas/Frutas.jpg  \n",
            "   creating: dataset/La_anonima/Garrapiñada/\n",
            "  inflating: dataset/La_anonima/Garrapiñada/Garrapiñada.jpg  \n",
            "   creating: dataset/La_anonima/Girasol/\n",
            "  inflating: dataset/La_anonima/Girasol/Girasol.jpg  \n",
            "   creating: dataset/La_anonima/Higos/\n",
            "  inflating: dataset/La_anonima/Higos/Higos.jpg  \n",
            "   creating: dataset/La_anonima/Hinojo/\n",
            "  inflating: dataset/La_anonima/Hinojo/Hinojo.jpg  \n",
            "   creating: dataset/La_anonima/Hongos/\n",
            "  inflating: dataset/La_anonima/Hongos/Hongos.jpg  \n",
            "   creating: dataset/La_anonima/Jengibre/\n",
            "  inflating: dataset/La_anonima/Jengibre/Jengibre.jpg  \n",
            "   creating: dataset/La_anonima/Kale/\n",
            "  inflating: dataset/La_anonima/Kale/Kale.jpg  \n",
            "   creating: dataset/La_anonima/Kiwi/\n",
            "  inflating: dataset/La_anonima/Kiwi/Kiwi.jpg  \n",
            "   creating: dataset/La_anonima/Lechuga/\n",
            "  inflating: dataset/La_anonima/Lechuga/Lechuga.jpg  \n",
            "   creating: dataset/La_anonima/Limón/\n",
            "  inflating: dataset/La_anonima/Limón/Limón.jpg  \n",
            "   creating: dataset/La_anonima/Mandarina/\n",
            "  inflating: dataset/La_anonima/Mandarina/Mandarina.jpg  \n",
            "   creating: dataset/La_anonima/Mango/\n",
            "  inflating: dataset/La_anonima/Mango/Mango.jpg  \n",
            "   creating: dataset/La_anonima/Manzana/\n",
            "  inflating: dataset/La_anonima/Manzana/Manzana.jpg  \n",
            "   creating: dataset/La_anonima/Maní/\n",
            "  inflating: dataset/La_anonima/Maní/Maní.jpg  \n",
            "   creating: dataset/La_anonima/Maíz/\n",
            "  inflating: dataset/La_anonima/Maíz/Maíz.jpg  \n",
            "   creating: dataset/La_anonima/Melón/\n",
            "  inflating: dataset/La_anonima/Melón/Melón.jpg  \n",
            "   creating: dataset/La_anonima/Mix/\n",
            "  inflating: dataset/La_anonima/Mix/Mix.jpg  \n",
            "   creating: dataset/La_anonima/Morrón/\n",
            "  inflating: dataset/La_anonima/Morrón/Morrón.jpg  \n",
            "   creating: dataset/La_anonima/Naranja/\n",
            "  inflating: dataset/La_anonima/Naranja/Naranja.jpg  \n",
            "   creating: dataset/La_anonima/Nueces/\n",
            "  inflating: dataset/La_anonima/Nueces/Nueces.jpg  \n",
            "   creating: dataset/La_anonima/Nuez/\n",
            "  inflating: dataset/La_anonima/Nuez/Nuez.jpg  \n",
            "   creating: dataset/La_anonima/Papa/\n",
            "  inflating: dataset/La_anonima/Papa/Papa.jpg  \n",
            "   creating: dataset/La_anonima/Papines/\n",
            "  inflating: dataset/La_anonima/Papines/Papines.jpg  \n",
            "   creating: dataset/La_anonima/Pasas/\n",
            "  inflating: dataset/La_anonima/Pasas/Pasas.jpg  \n",
            "   creating: dataset/La_anonima/Pelón/\n",
            "  inflating: dataset/La_anonima/Pelón/Pelón.jpg  \n",
            "   creating: dataset/La_anonima/Pepino/\n",
            "  inflating: dataset/La_anonima/Pepino/Pepino.jpg  \n",
            "   creating: dataset/La_anonima/Pera/\n",
            "  inflating: dataset/La_anonima/Pera/Pera.jpg  \n",
            "   creating: dataset/La_anonima/Perejil/\n",
            "  inflating: dataset/La_anonima/Perejil/Perejil.jpg  \n",
            "   creating: dataset/La_anonima/Pimienta/\n",
            "  inflating: dataset/La_anonima/Pimienta/Pimienta.jpg  \n",
            "   creating: dataset/La_anonima/Pipas/\n",
            "  inflating: dataset/La_anonima/Pipas/Pipas.jpg  \n",
            "   creating: dataset/La_anonima/Pistacho/\n",
            "  inflating: dataset/La_anonima/Pistacho/Pistacho.jpg  \n",
            "   creating: dataset/La_anonima/Pistachos/\n",
            "  inflating: dataset/La_anonima/Pistachos/Pistachos.jpg  \n",
            "   creating: dataset/La_anonima/Piña/\n",
            "  inflating: dataset/La_anonima/Piña/Piña.jpg  \n",
            "   creating: dataset/La_anonima/Plátano/\n",
            "  inflating: dataset/La_anonima/Plátano/Plátano.jpg  \n",
            "   creating: dataset/La_anonima/Pomelo/\n",
            "  inflating: dataset/La_anonima/Pomelo/Pomelo.jpg  \n",
            "   creating: dataset/La_anonima/Quinoa/\n",
            "  inflating: dataset/La_anonima/Quinoa/Quinoa.jpg  \n",
            "   creating: dataset/La_anonima/Rabanito/\n",
            "  inflating: dataset/La_anonima/Rabanito/Rabanito.jpg  \n",
            "   creating: dataset/La_anonima/Radiccio/\n",
            "  inflating: dataset/La_anonima/Radiccio/Radiccio.jpg  \n",
            "   creating: dataset/La_anonima/Radicheta/\n",
            "  inflating: dataset/La_anonima/Radicheta/Radicheta.jpg  \n",
            "   creating: dataset/La_anonima/Remolacha/\n",
            "  inflating: dataset/La_anonima/Remolacha/Remolacha.jpg  \n",
            "   creating: dataset/La_anonima/Repollo/\n",
            "  inflating: dataset/La_anonima/Repollo/Repollo.jpg  \n",
            "   creating: dataset/La_anonima/Ristra/\n",
            "  inflating: dataset/La_anonima/Ristra/Ristra.jpg  \n",
            "   creating: dataset/La_anonima/Rúcula/\n",
            "  inflating: dataset/La_anonima/Rúcula/Rúcula.jpg  \n",
            "   creating: dataset/La_anonima/Sandía/\n",
            "  inflating: dataset/La_anonima/Sandía/Sandía.jpg  \n",
            "   creating: dataset/La_anonima/Semilla/\n",
            "  inflating: dataset/La_anonima/Semilla/Semilla.jpg  \n",
            "   creating: dataset/La_anonima/Semillas/\n",
            "  inflating: dataset/La_anonima/Semillas/Semillas.jpg  \n",
            "   creating: dataset/La_anonima/Soja/\n",
            "  inflating: dataset/La_anonima/Soja/Soja.jpg  \n",
            "   creating: dataset/La_anonima/Sésamo/\n",
            "  inflating: dataset/La_anonima/Sésamo/Sésamo.jpg  \n",
            "   creating: dataset/La_anonima/Tomate/\n",
            "  inflating: dataset/La_anonima/Tomate/Tomate.jpg  \n",
            "   creating: dataset/La_anonima/Tomates/\n",
            "  inflating: dataset/La_anonima/Tomates/Tomates.jpg  \n",
            "   creating: dataset/La_anonima/Uvas/\n",
            "  inflating: dataset/La_anonima/Uvas/Uvas.jpg  \n",
            "   creating: dataset/La_anonima/Vegetales/\n",
            "  inflating: dataset/La_anonima/Vegetales/Vegetales.jpg  \n",
            "   creating: dataset/La_anonima/Zanahoria/\n",
            "  inflating: dataset/La_anonima/Zanahoria/Zanahoria.jpg  \n",
            "   creating: dataset/La_anonima/Zapallito/\n",
            "  inflating: dataset/La_anonima/Zapallito/Zapallito.jpg  \n",
            "   creating: dataset/La_anonima/Zapallo/\n",
            "  inflating: dataset/La_anonima/Zapallo/Zapallo.jpg  \n",
            "   creating: dataset/scripts/\n",
            "  inflating: dataset/scripts/normalizar_datos.py  \n",
            "  inflating: dataset/scripts/normalizar_imagenes.py  \n",
            "  inflating: dataset/scripts/script_carpetas.py  \n",
            "  inflating: dataset/VerdurasNormalizadas.csv  \n"
          ]
        }
      ],
      "source": [
        "# 4. Descomprimimos el archivo con el dataset.\n",
        "!unzip /content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS_wvmGByRim"
      },
      "source": [
        "##Fase 3: Preparación de los Datos\n",
        "En esta fase, se preparan los datos para su análisis y modelado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Cargar y preprocesar las imágenes.\n",
        "# Directorio base donde se encuentran las imágenes originales\n",
        "data_dir = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset'\n",
        "# Directorio base para las imágenes aumentadas\n",
        "augmented_data_dir = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset/augmented_data'\n",
        "\n",
        "# Configuración de la aumentación de datos\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Recorrer cada supermercado y cada fruta/verdura para generar imágenes aumentadas\n",
        "for supermarket in os.listdir(data_dir):\n",
        "    supermarket_path = os.path.join(data_dir, supermarket)\n",
        "    if os.path.isdir(supermarket_path):\n",
        "        for fruit in os.listdir(supermarket_path):\n",
        "            fruit_path = os.path.join(supermarket_path, fruit)\n",
        "            if os.path.isdir(fruit_path):\n",
        "                # Crear directorio para las imágenes aumentadas si no existe\n",
        "                save_to_dir = os.path.join(augmented_data_dir, supermarket, fruit)\n",
        "                os.makedirs(save_to_dir, exist_ok=True) # Asegúrate de que el directorio existe\n",
        "                for img_name in os.listdir(fruit_path):\n",
        "                    img_path = os.path.join(fruit_path, img_name)\n",
        "                    try:\n",
        "                        img = tf.keras.preprocessing.image.load_img(img_path)\n",
        "                        x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "                        x = x.reshape((1,) + x.shape)\n",
        "                        # Generar imágenes aumentadas\n",
        "                        i = 0\n",
        "                        for batch in datagen.flow(x, batch_size=1, save_to_dir=save_to_dir, save_prefix=fruit, save_format='jpeg'):\n",
        "                            i += 1\n",
        "                            if i > 50: # Generar 50 imágenes nuevas por imagen original\n",
        "                                break\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error procesando la imagen {img_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "q1BasVgV7zGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio base actual donde se encuentran las imágenes aumentadas\n",
        "current_augmented_data_dir = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset/augmented_data'\n",
        "\n",
        "# Nuevo directorio base donde se reorganizarán las imágenes aumentadas\n",
        "new_augmented_data_dir = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset/reorganized_data'\n",
        "os.makedirs(new_augmented_data_dir, exist_ok=True)  # Crear el directorio si no existe\n",
        "\n",
        "# Recorrer cada supermercado y cada fruta/verdura en el directorio actual\n",
        "for supermarket in os.listdir(current_augmented_data_dir):\n",
        "    supermarket_path = os.path.join(current_augmented_data_dir, supermarket)\n",
        "    if os.path.isdir(supermarket_path):\n",
        "        for fruit in os.listdir(supermarket_path):\n",
        "            fruit_path = os.path.join(supermarket_path, fruit)\n",
        "            if os.path.isdir(fruit_path):\n",
        "                for img_name in os.listdir(fruit_path):\n",
        "                    img_path = os.path.join(fruit_path, img_name)\n",
        "                    try:\n",
        "                        # Crear la nueva ruta para la imagen\n",
        "                        new_dir_path = os.path.join(new_augmented_data_dir, fruit, supermarket)\n",
        "                        os.makedirs(new_dir_path, exist_ok=True)  # Crear el directorio si no existe\n",
        "\n",
        "                        # Mover la imagen a la nueva ubicación\n",
        "                        new_img_path = os.path.join(new_dir_path, img_name)\n",
        "                        shutil.move(img_path, new_img_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error moviendo la imagen {img_path}: {e}\")\n",
        "\n",
        "print(\"Reorganización completa.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpKe5J3WFcCY",
        "outputId": "1ed299f5-e18a-468a-e951-66eb83edac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reorganización completa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kaLsR7bukLF"
      },
      "source": [
        "##Fase 4: Modelado\n",
        "En esta fase, se crean y entrenan los modelos de machine learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsJq5R3FZc2r"
      },
      "outputs": [],
      "source": [
        "# 6. Entrenamos y guardamos el modelo.\n",
        "# Directorios de datos\n",
        "base_dir = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/reorganized_dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Parámetros\n",
        "img_height, img_width = 150, 150\n",
        "batch_size = 32\n",
        "epochs = 75\n",
        "\n",
        "# Generadores de datos con aumentación\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2 # Utilizamos el 20% de los datos para validación\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Definir el modelo\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Guardar json con la clase de los datos\n",
        "with open('class_indices.json', 'w') as f:\n",
        "    json.dump(train_generator.class_indices, f)\n",
        "\n",
        "# Guardar el modelo final\n",
        "model.save('final_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fase 5: Evaluación\n",
        "En esta fase, se evalúa el modelo entrenado para asegurarse de que cumple con los requisitos del negocio."
      ],
      "metadata": {
        "id": "FuF6DtZginWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36qCIRGUExMw"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo guardado\n",
        "model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Model accuracy: {accuracy*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización con Matplotlib\n",
        "def plot_training_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Llamar a la función para plotear la historia del entrenamiento\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "HVG4LuZ2-W11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fase 6: Despliegue\n",
        "En esta fase, se implementa el modelo en un entorno de producción y se prepara para su uso.\n"
      ],
      "metadata": {
        "id": "o_5fpvOEi2yR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNt4Mei2ENrw"
      },
      "outputs": [],
      "source": [
        "# 8. Preparamos el archivo CSV para su procesamiento.\n",
        "# Ruta del archivo CSV\n",
        "prices_path = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/VerdurasNormalizadas.csv'\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "prices_df = pd.read_csv(prices_path)\n",
        "\n",
        "# Limpiar y convertir los precios a valores numéricos\n",
        "prices_df['Precio'] = prices_df['Precio'].str.replace(r'[^0-9]', '', regex=True)\n",
        "prices_df['Precio'] = pd.to_numeric(prices_df['Precio'], errors='coerce') / 100\n",
        "\n",
        "# Imprimir el DataFrame resultante\n",
        "print(prices_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Probamos el modelo y su funcionalidad."
      ],
      "metadata": {
        "id": "KA10zayei_LG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVnxP2L6-NqE"
      },
      "outputs": [],
      "source": [
        "# 9. Probamos el modelo y su funcionalidad.\n",
        "# Función para encontrar el supermercado más barato para un producto dado\n",
        "def supermercado_mas_barato(producto, prices_df):\n",
        "    precios_producto = prices_df[prices_df['Producto'] == producto]\n",
        "    supermercado_mas_barato = precios_producto.loc[precios_producto['Precio'].idxmin(), 'Supermercado']\n",
        "    precio_mas_bajo = precios_producto['Precio'].min()\n",
        "    return supermercado_mas_barato, precio_mas_bajo\n",
        "\n",
        "# Cargar el modelo previamente entrenado\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/ISPC-3°AÑO/asistente_compras_app/best_model.h5')\n",
        "\n",
        "# Función para clasificar una imagen y encontrar el supermercado más barato para esa categoría\n",
        "def clasificar_y_encontrar_supermercado(imagen_path, prices_df):\n",
        "    # Cargar la imagen y preprocesarla\n",
        "    img = load_img(imagen_path, target_size=(150, 150))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array.reshape((1,) + img_array.shape)\n",
        "    img_array /= 255.0 # Normalizar los valores de píxeles\n",
        "\n",
        "    # Clasificar la imagen utilizando el modelo\n",
        "    prediction = model.predict(img_array)\n",
        "    clase_predicha = np.argmax(prediction)\n",
        "\n",
        "    # Mapear la clase predicha al nombre de la fruta o verdura\n",
        "    if train_generator:\n",
        "        clases = train_generator.class_indices\n",
        "        frutas_verduras = {v: k for k, v in clases.items()}\n",
        "        producto = frutas_verduras[clase_predicha]\n",
        "\n",
        "        # Encontrar el supermercado más barato para el producto dado\n",
        "        supermercado, precio = supermercado_mas_barato(producto, prices_df)\n",
        "        return producto, supermercado, precio\n",
        "    else:\n",
        "        return None, None, None\n",
        "\n",
        "# Ruta de la imagen que deseas clasificar y comparar precios\n",
        "imagen_path = '/content/drive/MyDrive/ISPC-3°AÑO/AsistenteDeCompras/dataset/Carrefour/Coco/Coco.jpg'\n",
        "\n",
        "# Clasificar la imagen y encontrar el supermercado más barato\n",
        "producto, supermercado, precio = clasificar_y_encontrar_supermercado(imagen_path, prices_df)\n",
        "\n",
        "# Imprimir el resultado\n",
        "if producto:\n",
        "    print(f\"La imagen es de un(a) {producto}.\")\n",
        "    print(f\"El supermercado más barato para {producto} es {supermercado} con un precio de ${precio:.2f}.\")\n",
        "else:\n",
        "    print(\"No se pudo clasificar la imagen.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusión\n",
        "Este proyecto utiliza una combinación de técnicas de aprendizaje profundo y procesamiento de imágenes para crear un asistente de compras capaz de clasificar imágenes de frutas y verduras y encontrar el supermercado más barato para estos productos. Las herramientas y bibliotecas de Python proporcionan un entorno robusto para desarrollar, entrenar, evaluar y desplegar el modelo, permitiendo una implementación efectiva de la solución propuesta."
      ],
      "metadata": {
        "id": "vxjlr8s99DXZ"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}